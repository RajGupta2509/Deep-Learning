{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGUxH8hJO916"
      },
      "source": [
        "# Modeling Sequential Data Using Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04CIkiXGO917"
      },
      "source": [
        "# Introducing sequential data\n",
        "\n",
        "- What makes sequences unique, compared to other types of data, is that elements in a sequence appear in a certain order and are not independent of each other.\n",
        "\n",
        "- Typical machine learning algorithms\n",
        "for supervised learning assume that the input is independent and identically distributed (IID) data,\n",
        "which means that the training examples are mutually independent and have the same underlying distribution.\n",
        "\n",
        "- For a sensible example of sequences, consider time\n",
        "series data, where each example point, x(t), belongs to a particular time, t.\n",
        "\n",
        "Sequence modeling has many fascinating applications, such as `language translation` (for example,\n",
        "translating text from English to Hindi), `image captioning`, and `text generation`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential data versus time series data\n",
        "\n",
        "- Time series data is a special type of sequential data where each example is associated with a dimension for time.\n",
        "\n",
        "- In time series data, samples are taken at successive timestamps, and therefore, the time dimension determines the order among the data points. For example, stock prices and voice or speech records are time series data.\n",
        "\n",
        "- On the other hand, not all sequential data has the time dimension. For example, in text data or DNA\n",
        "sequences, the examples are ordered, but text or DNA does not qualify as time series data.\n",
        "\n",
        "**RNNs can also be used for time series data**"
      ],
      "metadata": {
        "id": "S7-2YDLhVaDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why RNNs?\n",
        "- The standard NN models that we have covered so far, such as multilayer perceptrons (`MLPs`) and `CNNs` for image data, assume that the training examples are independent\n",
        "of each other and thus do not incorporate ordering information. We can say that such models `do not have a memory of previously seen training examples`.\n",
        "- For instance, the samples are passed through\n",
        "the feedforward and backpropagation steps, and the weights are updated independently of the order\n",
        "in which the training examples are processed.\n",
        "\n",
        "- `RNNs`, by contrast, are designed for modeling sequences and are `capable of remembering past information and processing new events accordingly`, which is a clear advantage when working with sequence data"
      ],
      "metadata": {
        "id": "g-TwsamGWBQe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRflnyatO917"
      },
      "source": [
        "## The different categories of sequence modeling\n",
        "\n",
        "If neither the input nor output data represent sequences, then we are dealing with standard data, and we could simply use a multilayer perceptron (or another classification model) to model such data.\n",
        "\n",
        "However, `if either the input or output is a sequence`, the modeling task likely falls into one of these categories:\n",
        "\n",
        "1. **Many-to-One**: The `input data is a sequence, but the output is a fixed-size vector or scalar, not a sequence`.\n",
        "  - For example, in `sentiment analysis`, the input is text-based (for example, a movie review) and the output is a class label (for example, a label denoting whether a reviewer liked the movie).\n",
        "\n",
        "2. **One-to-Many**: `The input data is in standard format and not a sequence, but the output is a sequence`.\n",
        "  - An example of this category is `image captioning`—the input is an image and the output is an English phrase summarizing the content of that image.\n",
        "\n",
        "3. **Many-to-Many**: `Both the input and output arrays are sequences.` This category can be further\n",
        "divided based on `whether the input and output are synchronized`.\n",
        "- An example of a `synchronized`\n",
        "many-to-many modeling task is `video classification`, where each frame in a video is labeled.\n",
        "- An example of a `delayed` many-to-many modeling task would be `translating one language into another`. For instance, an entire English sentence must be read and processed by a machine before its translation into Hindi is produced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzqqxUTBO917"
      },
      "source": [
        "# RNNs for modeling sequences\n",
        "\n",
        "## Understanding the RNN looping mechanism\n",
        "\n",
        "- In a standard feedforward network, information flows from the input to the hidden layer, and then from\n",
        "the hidden layer to the output layer.\n",
        "\n",
        "- On the other hand, in an RNN, the hidden layer receives its input from both the input layer of the current time step and the hidden layer from the previous time step.\n",
        "\n",
        "- The flow of information in adjacent time steps in the hidden layer allows the network to have a memory of past events.\n",
        "\n",
        "- This flow of information is usually displayed as a loop, also known as a `recurrent edge` in graph notation, which is how this general RNN architecture got its name.\n",
        "\n",
        "- As we know, each hidden unit in a standard NN receives only one input—the net preactivation associated with the input layer.\n",
        "\n",
        "- In contrast, each hidden unit in an RNN receives two distinct sets of input—the `preactivation from the input layer` and the `activation of the same hidden layer from the previous time step, t-1`.\n",
        "\n",
        "- At the first time step, t=0, the hidden units are initialized to zeros or small random values. Then, at a\n",
        "time step where t>0, the hidden units receive their input from the data point at the current time, x(t),\n",
        "and the previous values of hidden units at t-1, indicated as $h^{(t–1)}$.\n",
        "\n",
        "Similarly, in the case of a multilayer RNN,\n",
        "- layer=1: Here, the hidden layer is represented as $h_1^{(t)}$ and it receives its input from the data point, x(t), and the hidden values in the same layer, but at the previous time step, $h_1^{(t-1)}$.\n",
        "- layer=2: The second hidden layer, $h_2^{(t)}$, receives its inputs from the outputs of the layer below\n",
        "at the current time step ($o_1^{(t)}$) and its own hidden values from the previous time step, $h_2^{(t-1)}$\n",
        "\n",
        "Since, in this case, each recurrent layer must receive a sequence as input, all the recurrent layers except\n",
        "the last one must return a sequence as output.\n",
        "The behavior of the last recurrent layer depends on the type of problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiDH_DZO917"
      },
      "source": [
        "## Computing Activations in an RNN\n",
        "\n",
        "The different weight matrices in a single-layer RNN are as follows:\n",
        "\n",
        "- $W_{xh}$: The weight matrix between the input, `x(t)`, and the hidden layer, `h`\n",
        "- $W_{hh}$: The weight matrix associated with the recurrent edge\n",
        "- $W_{ho}$: The weight matrix between the hidden layer and output layer\n",
        "- $W_h = [W_{xh}; W_{hh}]$\n",
        "\n",
        "For the hidden layer, the net input **$z_h(t)$** (pre-activation) is computed through a linear\n",
        "combination. That is, we compute the sum of the multiplications of the weight matrices with the\n",
        "corresponding vectors and add the bias unit:\n",
        "\n",
        "$$\n",
        "z_h(t) = W_{xh} \\cdot x(t) + W_{hh} \\cdot h(t-1) + b_h\n",
        "$$\n",
        "\n",
        "Then, the activations of the hidden units at time step `t` are calculated as follows:\n",
        "\n",
        "$$\n",
        "h(t) = \\sigma_h(z_h(t)) = \\sigma_h(W_{xh} \\cdot x(t) + W_{hh} \\cdot h(t-1) + b_h)\n",
        "$$\n",
        "\n",
        "Here, $( b_h )$ is the bias vector for the hidden units, and $(\\sigma_h)$ is the activation function of the hidden layer.\n",
        "\n",
        "If we use the concatenated weight matrix \\( W_h = [W_{xh}; W_{hh}] \\), then the formula for computing the hidden units becomes:\n",
        "\n",
        "$$\n",
        "h(t) = \\sigma_h\\left(\n",
        "\\begin{bmatrix}\n",
        "W_{xh} \\\\\n",
        "W_{hh}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x(t) \\\\\n",
        "h(t-1)\n",
        "\\end{bmatrix}\n",
        "+ b_h\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "Once the activations of the hidden units at the current time step are computed, the activations of the output units are calculated as:\n",
        "\n",
        "$$\n",
        "o(t) = \\sigma_o\\left( W_{ho} \\cdot h(t) + b_o \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b3GlJFwO918"
      },
      "source": [
        "## Hidden-recurrence vs. output-recurrence\n",
        "\n",
        "Now we know the recurrent networks in which the hidden layer has the recurrent property. However, there is an alternative model in which the recurrent connection comes from the output\n",
        "layer.\n",
        "\n",
        "In this case, the net activations from the output layer at the previous time step, $o^{t–1}$, can be added\n",
        "in one of two ways:\n",
        "- To the hidden layer at the current time step, $h^t$ (output-to-hidden recurrence).\n",
        "- To the output layer at the current time step, $o^t$ (output-to-output recurrence).\n",
        "\n",
        "The weights associated with the recurrent connection can\n",
        "be denoted for the hidden-to-hidden recurrence by $W_{hh}$, for the output-to-hidden recurrence by $W_{oh}$,\n",
        "and for the output-to-output recurrence by $W_{oo}$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the torch.nn module, a recurrent layer can be defined via RNN, which is similar to the\n",
        "hidden-to-hidden recurrence.\n",
        "\n",
        "In the following code, we will create a recurrent layer from RNN and perform a forward pass on an input sequence of length 3 to compute the output. We will also manually\n",
        "compute the forward pass and compare the results with those of RNN."
      ],
      "metadata": {
        "id": "qYhcTOYoobQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpVguQLUO918",
        "outputId": "d742b770-0444-41a8-d2f7-6ba7f3b8728a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_xh shape: torch.Size([2, 5])\n",
            "W_hh shape: torch.Size([2, 2])\n",
            "b_xh shape: torch.Size([2])\n",
            "b_hh shape: torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True)\n",
        "\n",
        "w_xh = rnn_layer.weight_ih_l0\n",
        "w_hh = rnn_layer.weight_hh_l0\n",
        "b_xh = rnn_layer.bias_ih_l0\n",
        "b_hh = rnn_layer.bias_hh_l0\n",
        "\n",
        "print('W_xh shape:', w_xh.shape)\n",
        "print('W_hh shape:', w_hh.shape)\n",
        "print('b_xh shape:', b_xh.shape)\n",
        "print('b_hh shape:', b_hh.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3g1OUEMO918",
        "outputId": "af1a2f7e-293c-4eb9-e6f1-e530323470f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time step 0 =>\n",
            "   Input           : [[1. 1. 1. 1. 1.]]\n",
            "   Hidden          : [[-0.4701929   0.58639044]]\n",
            "   Output (manual) : [[-0.3519801   0.52525216]]\n",
            "   RNN output      : [[-0.3519801   0.52525216]]\n",
            "\n",
            "Time step 1 =>\n",
            "   Input           : [[2. 2. 2. 2. 2.]]\n",
            "   Hidden          : [[-0.88883156  1.2364398 ]]\n",
            "   Output (manual) : [[-0.68424344  0.76074266]]\n",
            "   RNN output      : [[-0.68424344  0.76074266]]\n",
            "\n",
            "Time step 2 =>\n",
            "   Input           : [[3. 3. 3. 3. 3.]]\n",
            "   Hidden          : [[-1.3074702  1.8864892]]\n",
            "   Output (manual) : [[-0.8649416  0.9046636]]\n",
            "   RNN output      : [[-0.8649416  0.9046636]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
        "\n",
        "## output of the simple RNN:\n",
        "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
        "\n",
        "## manually computing the output:\n",
        "out_man = []\n",
        "for t in range(3):\n",
        "    xt = torch.reshape(x_seq[t], (1, 5))\n",
        "    print(f'Time step {t} =>')\n",
        "    print('   Input           :', xt.numpy())\n",
        "\n",
        "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh\n",
        "    print('   Hidden          :', ht.detach().numpy())\n",
        "\n",
        "    if t>0:\n",
        "        prev_h = out_man[t-1]\n",
        "    else:\n",
        "        prev_h = torch.zeros((ht.shape))\n",
        "\n",
        "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
        "    ot = torch.tanh(ot)\n",
        "    out_man.append(ot)\n",
        "    print('   Output (manual) :', ot.detach().numpy())\n",
        "    print('   RNN output      :', output[:, t].detach().numpy())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our manual forward computation, we used the hyperbolic tangent (tanh) activation function since it\n",
        "is also used in RNN (the default activation).\n",
        "\n",
        "As we can see from the printed results, the outputs from the\n",
        "manual forward computations exactly match the output of the RNN layer at each time step.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YM0S3fcXpqLu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI80XcYSO918"
      },
      "source": [
        "## The challenges of learning long-range interactions\n",
        "In Backpropagation through time (BPTT) during training RNNs,vanishing and exploding gradient problems arise in computing the gradients of a loss function\n",
        "\n",
        "In practice, there are at least three solutions to this problem:\n",
        "\n",
        "1. **`Gradient clipping`** - here, we specify a cut-off or threshold value for the gradients, and we assign this\n",
        "cut-off value to gradient values that exceed this value.\n",
        "2. **Truncated backpropagation through time (`TBPTT`)** - it simply limits the number of time steps that the signal can backpropagate after each forward pass.\n",
        "\n",
        "While both gradient clipping and TBPTT can solve the exploding gradient problem, the truncation limits\n",
        "the number of steps that the gradient can effectively flow back and properly update the weights.\n",
        "\n",
        "3. **`LSTM`**, has been more `successful in vanishing and exploding gradient problems` while modeling l`ong-range dependencies`\n",
        "through the use of `memory cells`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fril4J03O918"
      },
      "source": [
        "\n",
        "## Long Short-Term Memory cells\n",
        "- The building block of an LSTM is a `memory cell`, which essentially represents or `replaces the hidden layer` of standard RNNs.\n",
        "\n",
        "- In each memory cell, there is a `recurrent edge` that has the desirable weight, `w=1`, as we discussed, to\n",
        "overcome the vanishing and exploding gradient problems. The values associated with this recurrent\n",
        "edge are collectively called the `cell state`.\n",
        "\n",
        "- The cell state from the previous time step, C(t–1), is modified to get the cell state at the current time step, C(t), without being multiplied directly by any weight factor.\n",
        "\n",
        "- The flow of information in\n",
        "this memory cell is controlled by several computation units (often called gates).\n",
        "<br>\n",
        "\n",
        "In an LSTM cell, there are three different types of gates, which are known as the `forget gate`, the `input gate`, and the `output gate`:\n",
        "\n",
        "1. The **forget gate ($f_t$)** allows the memory cell to reset the cell state without growing indefinitely. In fact,\n",
        "the forget gate `decides which information is allowed to go through and which information to suppress`.\n",
        "\n",
        "2. The **input gate ($i_t$) and candidate value ($\\tilde{C_t}$)** are responsible for `updating the cell state`.\n",
        "\n",
        "3. The **output gate ($o_t$)** decides how to `update the values of hidden units`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjswaEVwO918"
      },
      "source": [
        "## Other advanced RNN models\n",
        "\n",
        "LSTMs provide a basic approach for modeling long-range dependencies in sequences. Yet, it is important to note that there are many variations of LSTMs\n",
        "\n",
        "Also there is a more recent approach, `gated recurrent unit (GRU)`, which was proposed in 2014. GRUs have a `simpler architecture` than LSTMs; therefore, they are `computationally more efficient`, while their performance in some tasks, such as polyphonic music modeling, is comparable to LSTMs.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}